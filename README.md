# BitNet-MCP
## Hybrid BitNet-MCP for Efficient Query Processing

### Build from source

> [!IMPORTANT]
> If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.

1. Clone the repo
```bash
git clone --recursive https://github.com/YIDEUNKIM/BitNet-MCP.git
cd BitNet-MCP
```
2. Install the dependencies
```bash
# (Recommended) Create a new conda environment
conda create -n bitnet-mcp python=3.11.13
conda activate bitnet-mcp

cd BitNet
pip install -r requirements.txt
```
3. Build the project
```bash
# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

```
<pre>
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
</pre>
## Usage
### Basic usage

```bash
# Run MCP Server
cd ..
python mcp_server.py
```

```bash
# Run inference with the quantized model
cd BitNet
python mcp_client.py
```
<pre>
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file (default="./models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf")
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text (default=256)
  -t THREADS, --threads THREADS
                        Number of threads to use(default=8)
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context (default=2048)
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text (default=0.8)
  -ngl, "--n-gpu-layers"
                        Number of layer to offload to GPU (dufault=0)
</pre>

### FAQ (Frequently Asked Questions)ğŸ“Œ 

#### Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?

**A:**
This is an issue introduced in recent version of llama.cpp. Please refer to this [commit](https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323) in the [discussion](https://github.com/abetlen/llama-cpp-python/issues/1942) to fix this issue.

#### Q2: How to build with clang in conda environment on windows?

**A:** 
Before building the project, verify your clang installation and access to Visual Studio tools by running:
```
clang -v
```

This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:
```
'clang' is not recognized as an internal or external command, operable program or batch file.
```

It indicates that your command line window is not properly initialized for Visual Studio tools.

â€¢ If you are using Command Prompt, run:
```
"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
```

â€¢ If you are using Windows PowerShell, run the following commands:
```
Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
```

These steps will initialize your environment and allow you to use the correct Visual Studio tools.


### git PR / commit

| **Type**         | **Description**                                   |
| ---------------- | ------------------------------------------------- |
| âœ¨**`Feat`**     | ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€                                  |
| ğŸ”¨**`Fix`**      | ë²„ê·¸ ìˆ˜ì •                                         |
| ğŸ“**`Docs`**     | ë¬¸ì„œ ì‘ì„± ë° ìˆ˜ì •                                 |
| â­ï¸**`Style`**   | ì½”ë“œ ìŠ¤íƒ€ì¼ ë° í¬ë§· ë³€ê²½(í•¨ìˆ˜ëª…/ë³€ìˆ˜ëª… ë³€ê²½ í¬í•¨) |
| ğŸ§ **`Refactor`** | ì½”ë“œ ë¦¬íŒ©í† ë§(ê¸°ëŠ¥ì€ ê°™ìœ¼ë‚˜ ë¡œì§ì´ ë³€ê²½ëœ ê²½ìš°)   |
| **`Test`**       | í…ŒìŠ¤íŠ¸ êµ¬í˜„                                       |
| ğŸ**`Chore`**    | ê¸°íƒ€ ìˆ˜ì • ì‚¬í•­(ex: gitignore, application.yml)    |
| ğŸ¨**`Design`**   | CSS ë“± ì‚¬ìš©ì UI ë””ìì¸ ë³€ê²½                      |
| **`Comment`**    | ì£¼ì„ ì‘ì„± ë° ìˆ˜ì •                                 |
| **`Rename`**     | íŒŒì¼/í´ë” ëª… ìˆ˜ì • ë° ì´ë™ ì‘ì—…                    |
| **`Remove`**     | íŒŒì¼/í´ë” ì‚­ì œ                                    |
| ğŸ”¥**`Hotfix`**   | ê¸‰í•˜ê²Œ ì¹˜ëª…ì ì¸ ë²„ê·¸ë¥¼ ê³ ì³ì•¼ í•˜ëŠ” ê²½ìš°           |
