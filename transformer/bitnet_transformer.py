import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import psutil
import os
import gc

# --- ë¦¬ì†ŒìŠ¤ ì¸¡ì • ë° ì¶œë ¥ í•¨ìˆ˜ ---
def measure_and_print_inference_usage(model, chat_input, max_new_tokens=50):
    """
    model.generate() í˜¸ì¶œ ì „í›„ì˜ ë©”ëª¨ë¦¬ ë³€í™”ë¥¼ ì¸¡ì •í•˜ì—¬
    ìˆœìˆ˜ ì¶”ë¡ ì— ì†Œìš”ëœ RAMê³¼ VRAM ì‚¬ìš©ëŸ‰ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    # 1. ì¶”ë¡  ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ ì¸¡ì •
    # -----------------------------------
    gc.collect() # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
    if torch.cuda.is_available():
        torch.cuda.empty_cache() # GPU ìºì‹œ ë¹„ìš°ê¸°
        torch.cuda.reset_peak_memory_stats() # ìµœëŒ€ ë©”ëª¨ë¦¬ í†µê³„ ì´ˆê¸°í™”
        
    process = psutil.Process(os.getpid())
    ram_before_gb = process.memory_info().rss / (1024 ** 3)
    
    vram_allocated_before_gb = 0
    if torch.cuda.is_available():
        vram_allocated_before_gb = torch.cuda.memory_allocated() / (1024 ** 3)

    print("--- ì¶”ë¡  ì‹œì‘ ì „ ---")
    print(f"RAM ì‚¬ìš©ëŸ‰: {ram_before_gb:.4f} GB")
    if torch.cuda.is_available():
        print(f"GPU VRAM í• ë‹¹ëŸ‰: {vram_allocated_before_gb:.4f} GB")
    print("-" * 20)

    # 2. ì¶”ë¡  ì‹¤í–‰
    # -----------------------------------
    chat_outputs = model.generate(**chat_input, max_new_tokens=max_new_tokens)
    
    # 3. ì¶”ë¡  ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ ì¸¡ì •
    # -----------------------------------
    ram_after_gb = process.memory_info().rss / (1024 ** 3)
    
    vram_allocated_after_gb = 0
    vram_peak_gb = 0
    if torch.cuda.is_available():
        vram_allocated_after_gb = torch.cuda.memory_allocated() / (1024 ** 3)
        vram_peak_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)

    print("--- ì¶”ë¡  ì™„ë£Œ í›„ ---")
    print(f"RAM ì‚¬ìš©ëŸ‰: {ram_after_gb:.4f} GB")
    if torch.cuda.is_available():
        print(f"GPU VRAM í• ë‹¹ëŸ‰ (í˜„ì¬): {vram_allocated_after_gb:.4f} GB")
        print(f"GPU VRAM í• ë‹¹ëŸ‰ (ìµœëŒ€): {vram_peak_gb:.4f} GB")
    print("-" * 20)
    
    # 4. ìˆœìˆ˜ ì¶”ë¡  ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê³„ì‚° ë° ì¶œë ¥
    # -----------------------------------
    ram_consumed = ram_after_gb - ram_before_gb
    vram_consumed = vram_peak_gb # reset_peak_memory_stats ì´í›„ì˜ ìµœëŒ“ê°’ì´ ìˆœìˆ˜ ì¶”ë¡  ì‚¬ìš©ëŸ‰
    
    print("âœ… ìˆœìˆ˜ ì¶”ë¡  ë‹¨ê³„ ë¦¬ì†ŒìŠ¤ ì†Œëª¨ëŸ‰")
    print(f"   - RAM ì¦ê°€ëŸ‰: {ram_consumed:.4f} GB")
    if torch.cuda.is_available():
        print(f"   - VRAM ì¦ê°€ëŸ‰ (Peak): {vram_consumed:.4f} GB")
    print("-" * 20)
    
    return chat_outputs


# --- ë©”ì¸ ì½”ë“œ ---
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì´ ë‹¨ê³„ëŠ” ì¸¡ì •ì—ì„œ ì œì™¸)
model_id = "microsoft/bitnet-b1.58-2B-4T"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="cpu"
)

# í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?"},
]
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
chat_input = tokenizer(prompt, return_tensors="pt").to(model.device)


# ì¶”ë¡  ë° ë¦¬ì†ŒìŠ¤ ì¸¡ì • í•¨ìˆ˜ í˜¸ì¶œ
# ----------------------------
print("\nğŸš€ ì¶”ë¡  ë‹¨ê³„ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
chat_outputs = measure_and_print_inference_usage(model, chat_input, max_new_tokens=512)


# ê²°ê³¼ ë””ì½”ë”© ë° ì¶œë ¥
response = tokenizer.decode(chat_outputs[0][chat_input['input_ids'].shape[-1]:], skip_special_tokens=True)
print("\nAssistant Response:", response)
