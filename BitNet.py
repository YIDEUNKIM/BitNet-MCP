import torch
import time
import psutil
import tracemalloc
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc

def format_bytes(bytes):
    """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes < 1024.0:
            return f"{bytes:.2f} {unit}"
        bytes /= 1024.0
    return f"{bytes:.2f} TB"

def get_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
    if torch.cuda.is_available():
        return {
            'allocated': torch.cuda.memory_allocated(),
            'reserved': torch.cuda.memory_reserved(),
            'max_allocated': torch.cuda.max_memory_allocated()
        }
    return None

def print_memory_info(stage):
    """ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥"""
    # CPU ë©”ëª¨ë¦¬
    process = psutil.Process()
    cpu_memory = process.memory_info()
    
    print(f"\n=== {stage} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ===")
    print(f"CPU RAM: {format_bytes(cpu_memory.rss)}")
    print(f"CPU Virtual: {format_bytes(cpu_memory.vms)}")
    
    # GPU ë©”ëª¨ë¦¬
    gpu_memory = get_gpu_memory()
    if gpu_memory:
        print(f"GPU Allocated: {format_bytes(gpu_memory['allocated'])}")
        print(f"GPU Reserved: {format_bytes(gpu_memory['reserved'])}")
        print(f"GPU Max Allocated: {format_bytes(gpu_memory['max_allocated'])}")
    else:
        print("GPU: ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")

def main():
    # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
    tracemalloc.start()
    
    # ì‹œì‘ ì‹œê°„
    total_start_time = time.time()
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
    print_memory_info("ì´ˆê¸° ìƒíƒœ")
    
    # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
    
    model_id = "microsoft/bitnet-b1.58-2B-4T"
    
    # 1. í† í¬ë‚˜ì´ì € ë¡œë”©
    print("\nğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer_start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer_time = time.time() - tokenizer_start
    print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ: {tokenizer_time:.2f}ì´ˆ")
    
    print_memory_info("í† í¬ë‚˜ì´ì € ë¡œë”© í›„")
    
    # 2. ëª¨ë¸ ë¡œë”©
    print("\nğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model_start = time.time()
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16
    )
    model_time = time.time() - model_start
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_time:.2f}ì´ˆ")
    
    print_memory_info("ëª¨ë¸ ë¡œë”© í›„")
    
    # 3. ì¶”ë¡  ì¤€ë¹„
    messages = [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user", "content": "How are you?"},
    ]
    
    # ì…ë ¥ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
    print("\nğŸ”„ ì…ë ¥ ì²˜ë¦¬ ì¤‘...")
    input_start = time.time()
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    chat_input = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_time = time.time() - input_start
    print(f"âœ… ì…ë ¥ ì²˜ë¦¬ ì™„ë£Œ: {input_time:.4f}ì´ˆ")
    
    print_memory_info("ì…ë ¥ ì²˜ë¦¬ í›„")
    
    # 4. ì¶”ë¡  ì‹¤í–‰
    print("\nğŸ”„ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
    inference_start = time.time()
    
    # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ í‰ê·  ì‹œê°„ ì¸¡ì •
    num_runs = 3
    total_inference_time = 0
    
    for i in range(num_runs):
        run_start = time.time()
        with torch.no_grad():  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
            chat_outputs = model.generate(**chat_input, max_new_tokens=50)
        run_time = time.time() - run_start
        total_inference_time += run_time
        print(f"  Run {i+1}: {run_time:.4f}ì´ˆ")
    
    avg_inference_time = total_inference_time / num_runs
    print(f"âœ… í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.4f}ì´ˆ")
    
    print_memory_info("ì¶”ë¡  ì™„ë£Œ í›„")
    
    # 5. ê²°ê³¼ ë””ì½”ë”©
    decode_start = time.time()
    response = tokenizer.decode(chat_outputs[0][chat_input['input_ids'].shape[-1]:], skip_special_tokens=True)
    decode_time = time.time() - decode_start
    print(f"âœ… ë””ì½”ë”© ì™„ë£Œ: {decode_time:.4f}ì´ˆ")
    
    # ì „ì²´ ì‹œê°„
    total_time = time.time() - total_start_time
    
    # Python ë©”ëª¨ë¦¬ ì¶”ì  ì •ë³´
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ¯ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼")
    print("="*50)
    print(f"í† í¬ë‚˜ì´ì € ë¡œë”©:     {tokenizer_time:.2f}ì´ˆ")
    print(f"ëª¨ë¸ ë¡œë”©:          {model_time:.2f}ì´ˆ")
    print(f"ì…ë ¥ ì²˜ë¦¬:          {input_time:.4f}ì´ˆ")
    print(f"í‰ê·  ì¶”ë¡  ì‹œê°„:      {avg_inference_time:.4f}ì´ˆ")
    print(f"ë””ì½”ë”©:            {decode_time:.4f}ì´ˆ")
    print(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„:      {total_time:.2f}ì´ˆ")
    
    print(f"\nPython ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    print(f"í˜„ì¬: {format_bytes(current)}")
    print(f"í”¼í¬: {format_bytes(peak)}")
    
    # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
    print_memory_info("ìµœì¢… ìƒíƒœ")
    
    print(f"\nğŸ’¬ AI ì‘ë‹µ: {response}")
    
    # ëª¨ë¸ ì‚¬ì´ì¦ˆ ì •ë³´
    param_count = sum(p.numel() for p in model.parameters())
    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
    
    print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"íŒŒë¼ë¯¸í„° ìˆ˜: {param_count:,}")
    print(f"ëª¨ë¸ í¬ê¸°: {format_bytes(param_size)}")
    
    # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
    input_tokens = chat_input['input_ids'].shape[1]
    output_tokens = chat_outputs.shape[1] - input_tokens
    tokens_per_second = output_tokens / avg_inference_time
    
    print(f"\nâš¡ ì²˜ë¦¬ëŸ‰:")
    print(f"ì…ë ¥ í† í°: {input_tokens}")
    print(f"ì¶œë ¥ í† í°: {output_tokens}")
    print(f"ìƒì„± ì†ë„: {tokens_per_second:.2f} tokens/sec")

if __name__ == "__main__":
    main()